<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://andongyang.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://andongyang.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-01T07:14:31+00:00</updated><id>https://andongyang.github.io/feed.xml</id><title type="html">blank</title><subtitle>Andong Yang&apos;s personal homepage. </subtitle><entry><title type="html">Tools for Finding Research Papers</title><link href="https://andongyang.github.io/blog/2024/ToolsForSearchingResearchPapers/" rel="alternate" type="text/html" title="Tools for Finding Research Papers"/><published>2024-11-20T00:00:00+00:00</published><updated>2024-11-20T00:00:00+00:00</updated><id>https://andongyang.github.io/blog/2024/ToolsForSearchingResearchPapers</id><content type="html" xml:base="https://andongyang.github.io/blog/2024/ToolsForSearchingResearchPapers/"><![CDATA[ <p>In every research field, there are countless papers published each year. In my area, robotics, I categorize papers into four groups based on their research direction:</p> <ol> <li>My Current Research Area – Topics directly related to my ongoing work in robotics.;</li> <li>Related Research Areas – Fields that are closely connected, such as AI/ML and Vision.</li> <li>Most Popular Research Areas – Areas that reflect industry needs or recent breakthroughs.</li> <li>Basic Research Areas – Foundational disciplines like mathematics and physics.</li> </ol> <p>This categorization helps me focus on what’s relevant while staying informed about broader developments.</p> <p><strong>Current Research Area - Robotics:</strong> My research focuses on robotics. To stay updated, I regularly review all papers published in key journals and conferences such as RSS, TRO, IJRR, and Science Robotics. For conferences like ICRA and IROS, which feature a large number of papers, I narrow my search by using keywords such as motion planning, swarm, multi-agent, field, or unstructured environments. For papers that seem essential, I use tools like <a href="https://www.connectedpapers.com">Connected Papers</a> to explore related works, trace the origins of an idea, and check for follow-up research.</p> <p><strong>Related Research Areas - AI/ML and Vision:</strong> For related fields like AI/ML and Vision, I focus on understanding trends and key developments. These fields often prioritize conferences over journals, so I pay close attention to major conferences, including AAAI, ICLR, ICML, NeurIPS, 3DV, CVPR, ECCV, ICCV, and WACV.</p> <p>Given the sheer volume of papers, it’s impossible to read all of them—or even skim their titles. Instead, I take the following approach:</p> <ol> <li>I start by reviewing the Best Paper and Best Paper Candidate for each conference. These papers often represent cutting-edge research and highlight the conference’s focus.</li> <li>I use tools like <a href="https://papercopilot.com">Paper Copilot</a> to find highly-rated accepted papers. I skim the titles of around first 150 papers and read those that are directly relevant or interesting to me. There are also some useful visualization tools, such as visualization of the development of research directions.</li> </ol> <p>Of course, there are still a lot of papers mey have someting we may interested. For these parts, I seek the help from social media and academic communities. For example, I follow IEEE Robotics and Automation Society groups and prominent researchers on platforms like X (formerly Twitter) and LinkedIn.</p> <p><strong>Most Popular and Basic Research Area:</strong> For popular research areas and foundational disciplines, my approach is less structured. I often browse sources like Nature News and Science Daily. Tools like <a href="https://openalex.org">OpenAlex</a> are also useful for exploring new methods or topics.</p> <p>I don’t depend heavily on recommendation algorithms, as I prefer maintaining control over what I read. For organizing papers, I use the file system of the operating system. It’s easy to back up.</p> <p>It’s easy to feel overwhelmed by the sheer number of papers and the fast pace of academic and industrial research. I remind myself that it’s okay to miss some papers—important ones will likely resurface in the future.</p> <p>Good luck with your research!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Managing the Overwhelming Number of Research Papers.]]></summary></entry><entry><title type="html">Videos of published paper</title><link href="https://andongyang.github.io/blog/2024/Video-archive/" rel="alternate" type="text/html" title="Videos of published paper"/><published>2024-07-05T00:00:00+00:00</published><updated>2024-07-05T00:00:00+00:00</updated><id>https://andongyang.github.io/blog/2024/Video%20archive</id><content type="html" xml:base="https://andongyang.github.io/blog/2024/Video-archive/"><![CDATA[<h3 id="speed-planning-based-on-terrain-aware-constraint-reinforcement-learning-in-rugged-environments">Speed Planning Based on Terrain-Aware Constraint Reinforcement Learning in Rugged Environments</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/Speed-Planning-Based.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> </div> <h3 id="f3dmp-foresighted-3d-motion-planning-of-mobile-robots-in-wild-environments">F3DMP: Foresighted 3D Motion Planning of Mobile Robots in Wild Environments</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/AndongYang-ICRA2024.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> </div> <h3 id="scoml-trajectory-planning-based-on-self-correcting-meta-reinforcement-learning-in-hybrid-terrain-for-mobile-robot">SCOML: Trajectory Planning Based on Self-Correcting Meta-Reinforcement Learning in Hybrid Terrain for Mobile Robot</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/AndongYang-IROS2024.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> </div> <h3 id="sms-mpc-adversarial-learning-based-simultaneous-prediction-control-with-single-model-for-mobile-robots">SMS-MPC: Adversarial Learning-based Simultaneous Prediction Control with Single Model for Mobile Robots</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/IROS2022.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> </div>]]></content><author><name></name></author><summary type="html"><![CDATA[This is an archive of videos of some published papers. The video archive will be continuously updated.]]></summary></entry><entry><title type="html">Global path planning based on satellite images</title><link href="https://andongyang.github.io/blog/2022/GlobalPathPlanningonSatelliteImage/" rel="alternate" type="text/html" title="Global path planning based on satellite images"/><published>2022-10-13T00:00:00+00:00</published><updated>2022-10-13T00:00:00+00:00</updated><id>https://andongyang.github.io/blog/2022/GlobalPathPlanningonSatelliteImage</id><content type="html" xml:base="https://andongyang.github.io/blog/2022/GlobalPathPlanningonSatelliteImage/"><![CDATA[<p>Based on satellite imagery, the Sat2Graph [1] method is used to automatically generate an initial road network. This initial road network is then manually corrected and refined to obtain the final road network result.</p> <p>On the basis of this final road network, traditional planning method (RRT* in OMPL) [2] are used to generate planning results, forming a dataset. This dataset is then used to train an imitation learning model [3].</p> <p>Finally, the imitation learning model is applied for planning. This imitation learning approach yields faster planning results at a large scale (20km) compared to using traditional planning method alone.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post2-2-480.webp 480w,/assets/img/post2-2-800.webp 800w,/assets/img/post2-2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post2-2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="vehicle" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post2-1-480.webp 480w,/assets/img/post2-1-800.webp 800w,/assets/img/post2-1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post2-1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="vehicle" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> RRT* planner visulization and Planning software GUI. </div> <p>[1] He, Songtao, et al. “Sat2graph: Road graph extraction through graph-tensor encoding.” Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXIV 16. Springer International Publishing, 2020.</p> <p>[2] Ioan A. Șucan, Mark Moll, Lydia E. Kavraki, The Open Motion Planning Library, IEEE Robotics &amp; Automation Magazine, 19(4):72–82, December 2012. https://ompl.kavrakilab.org</p> <p>[3] Ho, Jonathan, and Stefano Ermon. “Generative adversarial imitation learning.” Advances in neural information processing systems 29 (2016).</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Traditional RRT* and GAIl.]]></summary></entry><entry><title type="html">Building a self-driving car</title><link href="https://andongyang.github.io/blog/2021/BuildingASelfDrivingCar/" rel="alternate" type="text/html" title="Building a self-driving car"/><published>2021-11-05T00:00:00+00:00</published><updated>2021-11-05T00:00:00+00:00</updated><id>https://andongyang.github.io/blog/2021/BuildingASelfDrivingCar</id><content type="html" xml:base="https://andongyang.github.io/blog/2021/BuildingASelfDrivingCar/"><![CDATA[<p>Building a self-driving car that drives in the plateau environment. Participate in vehicle modification and solve the problem of no HD map, irregular path, and difficulty in manually correcting the path faced by global path planning for vehicle in plateau environment. Specific details include:</p> <ul> <li>Drawing the road network based on satellite images;</li> <li>Global path planning based on RRT* and A*.</li> <li>A visual interface based on QT is written to set target and check or modify the planning results.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/f51-480.webp 480w,/assets/img/f51-800.webp 800w,/assets/img/f51-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/f51.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="vehicle" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/f52-480.webp 480w,/assets/img/f52-800.webp 800w,/assets/img/f52-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/f52.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="vehicle" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Modified vehicle. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post1-1-480.webp 480w,/assets/img/post1-1-800.webp 800w,/assets/img/post1-1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post1-1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="vehicle" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/post1-2-480.webp 480w,/assets/img/post1-2-800.webp 800w,/assets/img/post1-2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/post1-2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="vehicle" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Path Planning software. </div>]]></content><author><name></name></author><summary type="html"><![CDATA[Sensor installation and path planning software design.]]></summary></entry></feed>